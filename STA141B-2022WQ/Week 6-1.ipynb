{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STA 141B Lecture 10\n",
    "\n",
    "February 08, 2022\n",
    "\n",
    "### Announcements\n",
    "\n",
    "* HW3 due next week\n",
    "\n",
    "### Topics\n",
    "\n",
    "* Web Scraping\n",
    "* debugging\n",
    "* (If time permits) Text Mining / Natural Language Processing\n",
    "\n",
    "### Datasets\n",
    "\n",
    "* [Craigslist Apartments](https://sacramento.craigslist.org/d/apts-housing-for-rent/search/apa)\n",
    "\n",
    "### References\n",
    "\n",
    "+ Web Scraping\n",
    "    * [MDN HTML Reference](https://developer.mozilla.org/en-US/docs/Web/HTML/Element)\n",
    "    * [XPath Diner](http://www.topswagcode.com/xpath/) -- an interactive XPath tutorial\n",
    "    * [CSS Diner](https://flukeout.github.io/) -- an interactive CSS Selector tutorial\n",
    "+ Natural Language Processing\n",
    "    * [Natural Language Processing with Python][nlpp], chapters 1-3. Beware: the print version is for Python 2.\n",
    "    * [Applied Text Analysis with Python][atap], chapters 1, 3.\n",
    "\n",
    "[PDSH]: https://jakevdp.github.io/PythonDataScienceHandbook/\n",
    "[ProGit]: https://git-scm.com/book/\n",
    "[nlpp]: https://www.nltk.org/book/\n",
    "[atap]: https://search.library.ucdavis.edu/primo-explore/fulldisplay?docid=01UCD_ALMA51320822340003126&context=L&vid=01UCD_V1&search_scope=everything_scope&tab=default_tab&lang=en_US"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 1 graded\n",
    "\n",
    "#### Summary\n",
    "\n",
    "* Average 8.85\n",
    "* 4 students got 10\n",
    "* 2 students recevied the extra credit (posted on Piazza)\n",
    "\n",
    "#### Remarks\n",
    "\n",
    "* 1.4: Common issues:\n",
    "    - didn't check if the set is empty first;\n",
    "    - didn’t exclude types other than `str`;\n",
    "    - didn’t include float type;\n",
    "    \n",
    "    ```\n",
    "    def better_mean(x):\n",
    "    if len(x) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Check for non-numeric elements in x.\n",
    "    for elt in x:\n",
    "        if isinstance(elt, int) or isinstance(elt, float):\n",
    "            pass\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    return mean(x)\n",
    "    ```\n",
    "\n",
    "* 1.6: Both `sort()` and `sorted()` can be used to sort a list:\n",
    "    - `sort()` will modify the list it is called on\n",
    "    - `sorted()` will create a new list containing a sorted version of the list it is given\n",
    "    \n",
    "* 1.7: Can you find initial __guesses__ to get both roots? Start a positive value to get the positive root and a negative value to get the negative root\n",
    "\n",
    "* 1.9: Sample code:\n",
    "```\n",
    "def fib(n):\n",
    "    if n == 0:\n",
    "        return \"0\"\n",
    "    \n",
    "    prev = \"0\"\n",
    "    curr = \"01\"\n",
    "    \n",
    "    while n > 1:\n",
    "        prev, curr = curr, curr + prev\n",
    "        n -= 1\n",
    "    \n",
    "    return curr, n\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall what we did last week, we web scraping [CUESA's Vegetable Seasons Chart](https://cuesa.org/eat-seasonally/charts/vegetables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our usual data science tools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Web scraping tools\n",
    "import lxml.html as lx\n",
    "import requests\n",
    "import requests_cache\n",
    "\n",
    "# requests_cache.install_cache(\"../craigslist1\")  #use cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Craigslist Apartments\n",
    "\n",
    "[Craigslist](https://www.craigslist.org/) is a popular website where people can post advertisements for free. We can use data from Craigslist to analyze the local rental market for apartments.\n",
    "\n",
    "Craigslist doesn't provide an API, so we have to scrape the data ourselves. Scraping Craigslist is the biggest challenge we've faced yet, since each ad is on a separate page.\n",
    "\n",
    "We can start by scraping the front page of the [apartments section](https://sacramento.craigslist.org/d/apts-housing-for-rent/search/apa) for links to individual ads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "craigslist_url = \"https://sacramento.craigslist.org/d/apts-housing-for-rent/search/apa\"\n",
    "\n",
    "response = requests.get(craigslist_url)\n",
    "response.raise_for_status()\n",
    "html = lx.fromstring(response.text)\n",
    "html.make_links_absolute(craigslist_url)\n",
    "\n",
    "# html.text_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`make_links_absolute(base_href)`: makes all links in the document absolute, assuming that base_href is the URL of the document. So if you pass base_href=\"http://localhost/foo/bar.html\" and there is a link to baz.html that will be rewritten as http://localhost/foo/baz.html.\n",
    "\n",
    "More explanation: [here](https://linuxtut.com/en/e03431c718b94d6304ff/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all <a> tags with class \"result-title\"\n",
    "links = html.xpath(\"//a[contains(@class, 'result-title')]/@href\")\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_page = html.xpath(\"//a[contains(@class, 'next')]/@href\")[0]\n",
    "next_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make it a function\n",
    "\n",
    "def scrape_front_page(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    html = lx.fromstring(response.text)\n",
    "    html.make_links_absolute(url)\n",
    "\n",
    "    # html\n",
    "\n",
    "    # Get all <a> tags with class \"result-title\"\n",
    "    links = html.xpath(\"//a[contains(@class, 'result-title')]/@href\")\n",
    "    \n",
    "    next_page = html.xpath(\"//a[contains(@class, 'next')]/@href\")[0]\n",
    "    \n",
    "    return next_page, links\n",
    "\n",
    "next_page, links = scrape_front_page(craigslist_url)\n",
    "#scrape_front_page(next_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(links[0])\n",
    "try:\n",
    "    response.raise_for_status()\n",
    "except:\n",
    "    print(\"The url couldn't be downloaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = lx.fromstring(response.text) # Parses an XML document or fragment from a string.\n",
    "html.text_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the price\n",
    "price = html.xpath(\"//*[contains(@class, 'price')]\")[0]\n",
    "price.text_content()\n",
    "\n",
    "html.cssselect(\".price\")[0].text_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using cssslect https://www.w3schools.com/cssref/css_selectors.asp\n",
    "html.cssselect(\"#titletextonly\")[0].text_content()\n",
    "html.cssselect(\"#postingbody\")[0].text_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html.xpath(\"//p[contains(@class, 'attrgroup')]/span\")[1].text_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = html.cssselect(\"#map\")[0]\n",
    "lon = coords.attrib.get(\"data-longitude\")\n",
    "lat = coords.attrib.get(\"data-latitude\")\n",
    "(lat, lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = html.cssselect(\"time.timeago\")[0]\n",
    "time = time.attrib.get(\"datetime\")\n",
    "time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_one_post(link):\n",
    "    response = requests.get(link)\n",
    "    try:\n",
    "        response.raise_for_status()\n",
    "    except:\n",
    "        print(\"The url couldn't be downloaded!\")\n",
    "\n",
    "    html = lx.fromstring(response.text) # Parses an XML document or fragment from a string.\n",
    "\n",
    "    #if len(html.cssselect(\".removed\")):\n",
    "        # Deleted post\n",
    "    #    return {\"price\": None}\n",
    "    \n",
    "    #try:\n",
    "    price = html.xpath(\"//*[contains(@class, 'price')]\")[0]\n",
    "    price = price.text_content()\n",
    "    # except IndexError:\n",
    "    #    price = None\n",
    "\n",
    "    # Alternative using CSS selectors:\n",
    "    # html.cssselect(\".price\") \n",
    "\n",
    "    title = html.cssselect(\"#titletextonly\")[0].text_content()\n",
    "\n",
    "    #html.cssselect(\"p.attrgroup span\")\n",
    "    attribs = [x.text_content() for x in html.xpath(\"//p[contains(@class, 'attrgroup')]/span\")]\n",
    "\n",
    "    text = html.cssselect(\"#postingbody\")[0].text_content()\n",
    "\n",
    "    #img = html.cssselect(\"div .first img\")[0]\n",
    "    #img_url = img.attrib.get(\"src\")\n",
    "    #img_resp = requests.get(img_url)\n",
    "    #img_resp.raise_for_status()\n",
    "    #img_resp.content\n",
    "\n",
    "    #img_url\n",
    "\n",
    "    # Next step: save image to file with open() and .write()\n",
    "    # Or we could use the wget package\n",
    "\n",
    "    coords = html.cssselect(\"#map\")[0]\n",
    "    lon = coords.attrib.get(\"data-longitude\")\n",
    "    lat = coords.attrib.get(\"data-latitude\")\n",
    "    (lat, lon)\n",
    "\n",
    "    time = html.cssselect(\"time.timeago\")[0]\n",
    "    time = time.attrib.get(\"datetime\")\n",
    "    time\n",
    "\n",
    "    return {\"text\": text, \"attribs\": attribs, \"lat\": lat, \"lon\": lon, \"time\": time, \"title\": title, \"price\": price}\n",
    "\n",
    "scrape_one_post(links[0])\n",
    "# scrape_one_post(links[1])\n",
    "# scrape_one_post(links[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = [scrape_one_post(u) for u in links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(links[54])\n",
    "response.raise_for_status()\n",
    "html = lx.fromstring(response.text)\n",
    "price = html.xpath(\"//*[contains(@class, 'price')]\")\n",
    "len(html.xpath(\"//*[contains(@class, 'price')]\")) # index out of range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for link in links:\n",
    "    posts = scrape_one_post(link)\n",
    "    i = i + 1\n",
    "    print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to improve our function?\n",
    "def scrape_one_post(link):\n",
    "    response = requests.get(link)\n",
    "    try:\n",
    "        response.raise_for_status()\n",
    "    except:\n",
    "        print(\"The url couldn't be downloaded!\")\n",
    "\n",
    "    html = lx.fromstring(response.text) # Parses an XML document or fragment from a string.\n",
    "\n",
    "    #if len(html.cssselect(\".removed\")):\n",
    "        # Deleted post\n",
    "    #    return {\"price\": None}\n",
    "    \n",
    "    #try:\n",
    "    if (len(html.xpath(\"//*[contains(@class, 'price')]\")) == 0):\n",
    "        return{\"price\": None}\n",
    "    else:\n",
    "        price = html.xpath(\"//*[contains(@class, 'price')]\")[0]\n",
    "        price = price.text_content()\n",
    "    # except IndexError:\n",
    "    #    price = None\n",
    "\n",
    "    # Alternative using CSS selectors:\n",
    "    # html.cssselect(\".price\") \n",
    "\n",
    "    title = html.cssselect(\"#titletextonly\")[0].text_content()\n",
    "\n",
    "    #html.cssselect(\"p.attrgroup span\")\n",
    "    attribs = [x.text_content() for x in html.xpath(\"//p[contains(@class, 'attrgroup')]/span\")]\n",
    "\n",
    "    text = html.cssselect(\"#postingbody\")[0].text_content()\n",
    "\n",
    "    #img = html.cssselect(\"div .first img\")[0]\n",
    "    #img_url = img.attrib.get(\"src\")\n",
    "    #img_resp = requests.get(img_url)\n",
    "    #img_resp.raise_for_status()\n",
    "    #img_resp.content\n",
    "\n",
    "    #img_url\n",
    "\n",
    "    # Next step: save image to file with open() and .write()\n",
    "    # Or we could use the wget package\n",
    "\n",
    "    coords = html.cssselect(\"#map\")[0]\n",
    "    lon = coords.attrib.get(\"data-longitude\")\n",
    "    lat = coords.attrib.get(\"data-latitude\")\n",
    "    (lat, lon)\n",
    "\n",
    "    time = html.cssselect(\"time.timeago\")[0]\n",
    "    time = time.attrib.get(\"datetime\")\n",
    "    time\n",
    "\n",
    "    return {\"text\": text, \"attribs\": attribs, \"lat\": lat, \"lon\": lon, \"time\": time, \"title\": title, \"price\": price}\n",
    "\n",
    "scrape_one_post(links[54])\n",
    "# scrape_one_post(links[1])\n",
    "# scrape_one_post(links[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = [scrape_one_post(u) for u in links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "A _natural language_ is a language people use to communicate, like English, Spanish, or Mandarin. These languages evolved over thousands of years and do not have simple, explicit rules.\n",
    "\n",
    "_Natural language processing_ (NLP) means using a computer to analyze, manipulate, or synthesize natural language. Some examples of NLP tasks are:\n",
    "* Translating from one language to another\n",
    "* Recognizing speech or handwriting\n",
    "* Tagging sentences with metadata, such as parts of speech (verbs, nouns, etc) or sentiment\n",
    "* Extracting information or computing statistics from text\n",
    "\n",
    "Compared to artificial languages like Python and XML, it's much more difficult to extract information from natural languages. NLP is a wide field; we only have time to learn the absolute basics. If you want to learn more, consider reading the entire [Natural Language Processing with Python][nlpp] book or taking a class in computational linguistics.\n",
    "\n",
    "[nlpp]: https://www.nltk.org/book/\n",
    "\n",
    "\n",
    "### The Python NLP Ecosystem\n",
    "\n",
    "There are lots of Python packages for NLP (try searching online)! A few popular ones are:\n",
    "\n",
    "* [Natural Language Tool Kit][nltk] (__nltk__) is the most popular. It's designed for learning and research, so it's well-documented and has lots of features.\n",
    "* [TextBlob][textblob] is a \"simplified\" package. It has a nicer interface than NLTK, but less features.\n",
    "* [SpaCy][spacy] is a \"production-ready\" package, and the fastest of all the packages listed here. Useful for working with large natural language datasets.\n",
    "* [gensim][gensim] is a package for creating topic models, which are a kind of statistical model that predict the topics of a text.\n",
    "\n",
    "We're going to learn __nltk__, but you might want to try some of the others if your project involves NLP.\n",
    "\n",
    "[Stanford's Core NLP][CoreNLP] library is at the cutting edge of NLP research. It's developed in Java, but several Python packages provide an interface (such as [pynlp][] and [stanford-corenlp][]).\n",
    "\n",
    "[nltk]: https://www.nltk.org/\n",
    "[spacy]: https://spacy.io/\n",
    "[textblob]: https://textblob.readthedocs.io/en/dev/\n",
    "[gensim]: https://radimrehurek.com/gensim/\n",
    "[CoreNLP]: https://stanfordnlp.github.io/CoreNLP/\n",
    "[pynlp]: https://github.com/sina-al/pynlp\n",
    "[stanford-corenlp]: https://github.com/Lynten/stanford-corenlp\n",
    "\n",
    "### Installing NLTK\n",
    "\n",
    "In an Anaconda Prompt (Win) or Terminal (MacOS & Linux), run:\n",
    "\n",
    "```shell\n",
    "conda install -c anaconda nltk\n",
    "```\n",
    "\n",
    "Then try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpora and Documents\n",
    "\n",
    "A _document_ is a single body text. When working with natural language data, documents are the unit of observation.\n",
    "\n",
    "What you choose as a document depends on the purpose of your analysis. If you're studying how people react to news on Twitter, it makes sense to use individual tweets as documents. If you're studying how animals are portrayed in 19th-century literature, you could use individual novels as documents.\n",
    "\n",
    "A _corpus_ is a collection of documents. In other words, a corpus is a dataset.\n",
    "\n",
    "__nltk__ provides some example corpora in the `nltk.corpus` submodule. The documentation gives a [complete list](http://www.nltk.org/nltk_data/). Most have to be downloaded with `nltk.download()` before use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.corpus\n",
    "\n",
    "# Download books from Project Gutenberg\n",
    "nltk.download(\"gutenberg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.fileids()` method lists the documents in a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.raw()` method returns the raw text for a single document. Specify the document by its file ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice = nltk.corpus.gutenberg.raw(\"carroll-alice.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "A _token_ is a sequence of characters to be treated as a group. Tokens are the unit of analysis for an indvidual document.\n",
    "\n",
    "Tokens can represent paragraphs, sentences, words, or something else. Most of the time, tokens will be words.\n",
    "\n",
    "When you analyze a document, the first step will usually be to split the document into tokens. Functions that do this are called _tokenizers_, and this process is called _tokenization_.\n",
    "\n",
    "The `nltk.sent_tokenize()` function splits a document into sentences, and the `nltk.word_tokenize()` function splits a document into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.sent_tokenize(alice)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.word_tokenize(alice)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpora also have `.sents()` and `.word()` methods for tokenization. These methods are specialized to the corpus, so they sometimes use the different strategies than `sent_tokenize()` and `word_tokenize()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.gutenberg.sents(\"carroll-alice.txt\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.gutenberg.words(\"carroll-alice.txt\")[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strings, String Methods, and Regular Expressions\n",
    "\n",
    "How does word tokenization actually work?\n",
    "\n",
    "The simplest strategy is to split at whitespace. You can do this with Python's built-in string methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting on whitespace doesn't handle punctuation. You can use regular expressions to split on more complex patterns. Python's built-in __re__ module provides regular expression functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "re.split(\"[ ,.:;!']\", alice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we also want to split at newlines?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
