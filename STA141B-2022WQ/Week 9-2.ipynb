{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "998f1fe7",
   "metadata": {},
   "source": [
    "# STA 141B Lecture 18\n",
    "March 02, 2022\n",
    "\n",
    "### Annoucement\n",
    "* Final presentation\n",
    "* Presentation grading link [here](https://docs.google.com/forms/d/e/1FAIpQLSfNJ5o9Ye26MUthZoGpVsxb6si1O1lruWh9CtNbZTHZFGd4vg/viewform)\n",
    "* Please complete the course evaluation survey. \n",
    "  * For the question in the survey \"provide additional comments below\", I __highly encourage__ you to comment on \n",
    "      - which part of the course you __liked the most__ and would like me to keep for teaching this course in the future;\n",
    "      - which part of the course you __did not like__ and think can be changed in the future.\n",
    "  \n",
    "### Topics\n",
    "\n",
    "* Parallel computing in python\n",
    "* Spark\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16f839b",
   "metadata": {},
   "source": [
    "### Parallel computing in python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc078db",
   "metadata": {},
   "source": [
    "#### Computer architecture\n",
    "\n",
    "* Each computer can have multiple CPUs, each CPU has multiple cores (e.g., two quad-core CPUs)\n",
    "* All the CPUs are connected to memory (e.g., 64G memory)\n",
    "* CPU cores can execute in parallel\n",
    "\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"fig/fig1.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74092ac6",
   "metadata": {},
   "source": [
    "#### Multicore programming\n",
    "\n",
    "* Execute tasks simultaneously on many CPU cores\n",
    "\n",
    "<div>\n",
    "<img src=\"fig/fig2.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b86408",
   "metadata": {},
   "source": [
    "A __thread__ is a path of execution within a process. A process can contain multiple threads.\n",
    "\n",
    "* In single-threaded processes, the process contains one thread.\n",
    "\n",
    "* In multithreaded processes, the process contains more than one thread, and the process is accomplishing a number of things at the same time. \n",
    "\n",
    "* Threads are sometimes called lightweight processes because they have their own stack but can access shared data. \n",
    "\n",
    "<div>\n",
    "<img src=\"fig/fig3.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7f5de5",
   "metadata": {},
   "source": [
    "A __processes__ are \"share nothing\", which are independent executing without sharing memory or state. This makes it easier to turn into a distributed application.\n",
    "\n",
    "<div>\n",
    "<img src=\"fig/fig4.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c671d7f",
   "metadata": {},
   "source": [
    "#### Python thread\n",
    "\n",
    "* In python, package \"threading\", which allows you to have different parts of your program run concurrently and can simplify your design. [Reference](https://realpython.com/intro-to-python-threading/#:~:text=Python%20threading%20allows%20you%20to,this%20tutorial%20is%20for%20you!)\n",
    "* Unfortunately, python only allows a single thread to be executing at once - due to GIL (global interpreter lock)\n",
    "* Usually no or little speedup - only useful when you want to interleave I/O and CPU execution\n",
    "\n",
    "#### Python process\n",
    "\n",
    "* Package [multiprocessing](https://docs.python.org/3/library/multiprocessing.html)\n",
    "* You can create multiple processes\n",
    "    - Automatically run on multiple CPU cores\n",
    "    - Default no shared memory, each process has its own memory space (larger memory overhead)\n",
    "    - Can declare some part of memory to be shared (but often harder to use)\n",
    "* You can also check some tutorials: [T1](http://sebastianraschka.com/Articles/2014_multiprocessing.html) [T2](https://pymotw.com/2/multiprocessing/basics.html)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c213a58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "mp.set_start_method(\"fork\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64c5e0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World 0\n",
      "\n",
      "Hello World 1\n",
      "Hello World 2\n",
      "\n",
      "\n",
      "Hello World 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def helloworld(x):\n",
    "    print('Hello World %d\\n'%x)\n",
    "\n",
    "# set up a list of processes\n",
    "plist = []\n",
    "\n",
    "for x in range(4):\n",
    "    plist.append(mp.Process(target=helloworld, args = (x, )))\n",
    "\n",
    "# Run processes\n",
    "for p in plist:\n",
    "    p.start()\n",
    "\n",
    "# exist the completed processes\n",
    "for p in plist:\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a3c0d5",
   "metadata": {},
   "source": [
    "Multiprocessing supports two types of communication channels between processes:\n",
    "* Queue\n",
    "* Pool\n",
    "\n",
    "#### Queue\n",
    "\n",
    "* The Queue class in multiprocessing module of Python Standard Library provides a mechanism to pass data between a parent process and the descendent processes of it.\n",
    "* mp.Queue is a concurrent and \"first in first out\" data structure\n",
    "\n",
    "<div>\n",
    "<img src=\"fig/fig5.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "</div>\n",
    "\n",
    "* Can be used to communicate, or gather the results from the processes\n",
    "* Queue.put(): insert an object to the end of queue\n",
    "* Queue.get(): remove the first element in the queue\n",
    "* Queue.empty: check whether the queue is empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c801cc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42, None, 'hello']\n"
     ]
    }
   ],
   "source": [
    "def f(q):\n",
    "    q.put([42, None, 'hello'])\n",
    "    \n",
    "if __name__ == '__main__' :\n",
    "    q = mp.Queue()\n",
    "    p = mp.Process(target = f, args = (q, ))\n",
    "    p.start()\n",
    "    print(q.get()) \n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd6e1f4",
   "metadata": {},
   "source": [
    "\"if \\_\\_name\\_\\_ == '\\_\\_main\\_\\_'\" is used to allow or prevent parts of code from being run when the modules are imported. When the Python interpreter reads a file, the \\_\\_name\\_\\_ variable is set as \\_\\_main\\_\\_ if the module being run, or as the module's name if it is imported. \n",
    "\n",
    "See more details [here](https://stackoverflow.com/questions/419163/what-does-if-name-main-do)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd21e07",
   "metadata": {},
   "source": [
    "#### Pool\n",
    "\n",
    "* mp.pool is another and more convenient approach for parallel processing in python.\n",
    "* Use mp.Pool(processes=4) to create 4 processes\n",
    "* Use $[r_1, r_2, \\dots, r_k]$ = pool.map(f, $[x_1, x_2, \\dots, x_k]$) to run multiple processes and get the results\n",
    "     - $f$ is the function to run for the processes\n",
    "     - $[x_1, x_2, \\dots, x_k]$ are the input arguments we want to run for the function (this is a size $k$ list)\n",
    "     - $[r_1, r_2, \\dots, r_k]$ are the output arguments we get after running the functions for each input (this is a size $k$ list)\n",
    "     - $k$ may be larger than number of processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc1eb1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16, 25]\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return x*x\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with mp.Pool(4) as p:\n",
    "        print(p.map(f, [1, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8baf23",
   "metadata": {},
   "source": [
    "## Spark\n",
    "\n",
    "Spark is great for scaling up data science tasks and workloads. \n",
    "\n",
    "To run spark, you need to use Spark data frames and libraries that operate on these data structures, you can scale to massive data sets that distribute across a cluster.\n",
    "\n",
    "* __Native Spark__: if youâ€™re using Spark data frames and libraries (e.g. MLlib), then your code will be parallelized and distributed natively by Spark\n",
    "* __Thread Pools__: The multiprocessing library can be used to run concurrent Python threads, and even perform operations with Spark data frames.\n",
    "* __Pandas UDFs__: A new feature in Spark that enables parallelized processing on Pandas data frames within a Spark environment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a3596a",
   "metadata": {},
   "source": [
    "#### Single thread example \n",
    "\n",
    "Reference [here](https://towardsdatascience.com/3-methods-for-parallelization-in-spark-6a1a4333b473)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7206dfe2",
   "metadata": {},
   "source": [
    "Goal: using Boston housing data set to build a regression model for predicting house prices using 13 different features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b10c5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>0.29819</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.504</td>\n",
       "      <td>7.686</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3.3751</td>\n",
       "      <td>8.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>17.4</td>\n",
       "      <td>377.51</td>\n",
       "      <td>3.92</td>\n",
       "      <td>46.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.14455</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.172</td>\n",
       "      <td>96.1</td>\n",
       "      <td>5.9505</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>19.15</td>\n",
       "      <td>27.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>0.36920</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.90</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.544</td>\n",
       "      <td>6.567</td>\n",
       "      <td>87.3</td>\n",
       "      <td>3.6023</td>\n",
       "      <td>4.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>18.4</td>\n",
       "      <td>395.69</td>\n",
       "      <td>9.28</td>\n",
       "      <td>23.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.06860</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.445</td>\n",
       "      <td>7.416</td>\n",
       "      <td>62.5</td>\n",
       "      <td>3.4952</td>\n",
       "      <td>2.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>6.19</td>\n",
       "      <td>33.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.05660</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.41</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.489</td>\n",
       "      <td>7.007</td>\n",
       "      <td>86.3</td>\n",
       "      <td>3.4217</td>\n",
       "      <td>2.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.50</td>\n",
       "      <td>23.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "228  0.29819   0.0   6.20   0.0  0.504  7.686  17.0  3.3751  8.0  307.0   \n",
       "7    0.14455  12.5   7.87   0.0  0.524  6.172  96.1  5.9505  5.0  311.0   \n",
       "314  0.36920   0.0   9.90   0.0  0.544  6.567  87.3  3.6023  4.0  304.0   \n",
       "99   0.06860   0.0   2.89   0.0  0.445  7.416  62.5  3.4952  2.0  276.0   \n",
       "88   0.05660   0.0   3.41   0.0  0.489  7.007  86.3  3.4217  2.0  270.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  target  \n",
       "228     17.4  377.51   3.92    46.7  \n",
       "7       15.2  396.90  19.15    27.1  \n",
       "314     18.4  395.69   9.28    23.8  \n",
       "99      18.0  396.90   6.19    33.2  \n",
       "88      17.8  396.90   5.50    23.6  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# load the boston data set\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "\n",
    "# convert to a Pandas Data Frame\n",
    "boston_pd = pd.DataFrame(data= np.c_[boston['data'],boston['target']], \n",
    "              columns= np.append(boston['feature_names'], 'target')).sample(frac=1)\n",
    "print(boston_pd.shape)\n",
    "boston_pd.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf4a3ec",
   "metadata": {},
   "source": [
    "* Split the data set into _training_ and _testing_ groups and separate the features from the labels for each group. \n",
    "* Use the LinearRegression class to fit the training data set and create predictions for the test data set. \n",
    "* Calculate the correlation coefficient between the actual and predicted house prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fab46108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-sqaured: 0.7714906558491852\n",
      "MAE: 3.3652865859490313\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "np.random.seed(2022) # set seed\n",
    "\n",
    "# split into data and label arrays \n",
    "y = boston_pd['target']\n",
    "X = boston_pd.drop(['target'], axis=1)\n",
    "\n",
    "# create training (~80%) and test data sets\n",
    "X_train = X[:400]\n",
    "X_test = X[400:]\n",
    "y_train = y[:400]\n",
    "y_test = y[400:]\n",
    "\n",
    "# train a classifier \n",
    "lr = LinearRegression()\n",
    "model = lr.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# error metrics\n",
    "r = pearsonr(y_pred, y_test)\n",
    "mae = sum(abs(y_pred - y_test))/len(y_test) \n",
    "print('R-sqaured: ' + str(r[0]**2))\n",
    "print(\"MAE: \" + str(mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3489564",
   "metadata": {},
   "source": [
    "#### Native Spark\n",
    "\n",
    "If you use Spark data frames and libraries, then Spark will natively parallelize and distribute your task. First, weâ€™ll need to convert the Pandas data frame to a Spark data frame, and then transform the features into the spark vector representation required for MLlib. The snippet below shows how to perform this task for the housing data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90516978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /Users/yuchien/opt/anaconda3/lib/python3.8/site-packages (3.2.1)\n",
      "Requirement already satisfied: py4j==0.10.9.3 in /Users/yuchien/opt/anaconda3/lib/python3.8/site-packages (from pyspark) (0.10.9.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b9ea024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(CRIM=0.29819, ZN=0.0, INDUS=6.2, CHAS=0.0, NOX=0.504, RM=7.686, AGE=17.0, DIS=3.3751, RAD=8.0, TAX=307.0, PTRATIO=17.4, B=377.51, LSTAT=3.92, target=46.7),\n",
       " Row(CRIM=0.14455, ZN=12.5, INDUS=7.87, CHAS=0.0, NOX=0.524, RM=6.172, AGE=96.1, DIS=5.9505, RAD=5.0, TAX=311.0, PTRATIO=15.2, B=396.9, LSTAT=19.15, target=27.1),\n",
       " Row(CRIM=0.3692, ZN=0.0, INDUS=9.9, CHAS=0.0, NOX=0.544, RM=6.567, AGE=87.3, DIS=3.6023, RAD=4.0, TAX=304.0, PTRATIO=18.4, B=395.69, LSTAT=9.28, target=23.8),\n",
       " Row(CRIM=0.0686, ZN=0.0, INDUS=2.89, CHAS=0.0, NOX=0.445, RM=7.416, AGE=62.5, DIS=3.4952, RAD=2.0, TAX=276.0, PTRATIO=18.0, B=396.9, LSTAT=6.19, target=33.2),\n",
       " Row(CRIM=0.0566, ZN=0.0, INDUS=3.41, CHAS=0.0, NOX=0.489, RM=7.007, AGE=86.3, DIS=3.4217, RAD=2.0, TAX=270.0, PTRATIO=17.8, B=396.9, LSTAT=5.5, target=23.6)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "#sc = SparkContext('local')\n",
    "#spark = SparkSession(sc)\n",
    "\n",
    "# convert to a Spark data frame\n",
    "boston_sp = spark.createDataFrame(boston_pd)\n",
    "display(boston_sp.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00c384c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(CRIM=0.29819, ZN=0.0, INDUS=6.2, CHAS=0.0, NOX=0.504, RM=7.686, AGE=17.0, DIS=3.3751, RAD=8.0, TAX=307.0, PTRATIO=17.4, B=377.51, LSTAT=3.92, target=46.7),\n",
       " Row(CRIM=0.14455, ZN=12.5, INDUS=7.87, CHAS=0.0, NOX=0.524, RM=6.172, AGE=96.1, DIS=5.9505, RAD=5.0, TAX=311.0, PTRATIO=15.2, B=396.9, LSTAT=19.15, target=27.1),\n",
       " Row(CRIM=0.3692, ZN=0.0, INDUS=9.9, CHAS=0.0, NOX=0.544, RM=6.567, AGE=87.3, DIS=3.6023, RAD=4.0, TAX=304.0, PTRATIO=18.4, B=395.69, LSTAT=9.28, target=23.8),\n",
       " Row(CRIM=0.0686, ZN=0.0, INDUS=2.89, CHAS=0.0, NOX=0.445, RM=7.416, AGE=62.5, DIS=3.4952, RAD=2.0, TAX=276.0, PTRATIO=18.0, B=396.9, LSTAT=6.19, target=33.2),\n",
       " Row(CRIM=0.0566, ZN=0.0, INDUS=3.41, CHAS=0.0, NOX=0.489, RM=7.007, AGE=86.3, DIS=3.4217, RAD=2.0, TAX=270.0, PTRATIO=17.8, B=396.9, LSTAT=5.5, target=23.6)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "# sc = SparkContext('local')\n",
    "# spark = SparkSession(sc)\n",
    "\n",
    "# convert to a Spark data frame\n",
    "boston_sp = spark.createDataFrame(boston_pd)\n",
    "display(boston_sp.take(5))\n",
    "\n",
    "np.random.seed(2022)\n",
    "\n",
    "# split into training and test spark data frames\n",
    "boston_train = spark.createDataFrame(boston_pd[:400])\n",
    "boston_test = spark.createDataFrame(boston_pd[400:])\n",
    "\n",
    "# convert to vector representation for MLlib\n",
    "assembler = VectorAssembler(inputCols = boston_train.schema.names[:(boston_pd.shape[1] - 1)], outputCol=\"features\" )\n",
    "boston_train = assembler.transform(boston_train).select('features', 'target') \n",
    "boston_test = assembler.transform(boston_test).select('features', 'target') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b92dc863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorAssembler_e822f6418b09"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assembler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a015f3",
   "metadata": {},
   "source": [
    "Now that we have the data prepared in the Spark format, we can use MLlib to perform parallelized fitting and model prediction. The snippet below shows how to instantiate and train a linear regression model and calculate the correlation coefficient for the estimated house prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1f526dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-sqaured: 0.7680557734660175\n"
     ]
    }
   ],
   "source": [
    "# linear regresion with Spark\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# linear regression \n",
    "lr = LinearRegression(maxIter=10, regParam=0.1, \n",
    "                      elasticNetParam=0.5, labelCol=\"target\")\n",
    "\n",
    "# Fit the model\n",
    "model = lr.fit(boston_train)\n",
    "boston_pred = model.transform(boston_test)\n",
    "\n",
    "# calculate results \n",
    "r = boston_pred.stat.corr(\"prediction\", \"target\")\n",
    "print(\"R-sqaured: \" + str(r**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416f9ec1",
   "metadata": {},
   "source": [
    "We now have a model fitting and prediction task that is parallelized. However, what if we also want to concurrently try out different hyperparameter configurations?\n",
    "\n",
    "* use the CrossValidator class that performs this operation natively in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ef84fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-sqaured: 0.7714906558491647\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "crossval = CrossValidator(estimator=LinearRegression(labelCol = \"target\"),  \n",
    "                         estimatorParamMaps=ParamGridBuilder().addGrid(\n",
    "                           LinearRegression.elasticNetParam, [0, 0.5, 1.0]).build(),\n",
    "                         evaluator=RegressionEvaluator(\n",
    "                           labelCol = \"target\", metricName = \"r2\"),\n",
    "                         numFolds=10)\n",
    "\n",
    "# cross validate the model and select the best fit\n",
    "cvModel = crossval.fit(boston_train) \n",
    "model = cvModel.bestModel\n",
    "\n",
    "# calculate results \n",
    "boston_pred = model.transform(boston_test)\n",
    "r = boston_pred.stat.corr(\"prediction\", \"target\")\n",
    "print(\"R-sqaured: \" + str(r**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792d6b96",
   "metadata": {},
   "source": [
    "#### Thread Pools\n",
    "\n",
    "One of the ways that you can achieve parallelism in Spark without using Spark data frames is by using the multiprocessing library.\n",
    "\n",
    "The library provides a thread abstraction that you can use to create concurrent threads of execution. However, by default all of your code will run on the driver node. \n",
    "\n",
    "The snippet below shows how to create a set of threads that will run in parallel, are return results for different hyperparameters for a random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72e486fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[10, 0.8468220090058051], [20, 0.8505399421144217], [50, 0.8480309239742657]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sklearn version \n",
    "from sklearn.ensemble import RandomForestRegressor as RFR\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "# allow up to 5 concurrent threads\n",
    "pool = ThreadPool(5)\n",
    "\n",
    "# hyperparameters to test out (n_trees)\n",
    "parameters = [ 10, 20, 50]\n",
    "\n",
    "# define a function to train a RF model and return metrics \n",
    "def sklearn_random_forest(trees, X_train, X_test, y_train, y_test):\n",
    "\n",
    "    # train a random forest regressor with the specified number of trees\n",
    "    rf= RFR(n_estimators = trees)\n",
    "    model = rf.fit(X_train, y_train)\n",
    "\n",
    "    # make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    r = pearsonr(y_pred, y_test)\n",
    "\n",
    "    # return the number of trees, and the R value \n",
    "    return [trees, r[0]**2]  \n",
    "\n",
    "# run the tasks \n",
    "pool.map(lambda trees: sklearn_random_forest(trees, X_train,\n",
    "                                           X_test, y_train, y_test), parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd4ada5",
   "metadata": {},
   "source": [
    "This approach works by using the map function on a pool of threads. The map function takes a lambda expression and array of values as input, and invokes the lambda expression for each of the values in the array. Once all of the threads complete, the output displays the hyperparameter value (n_estimators) and the R-squared result for each thread.\n",
    "\n",
    "Using thread pools this way is dangerous, because all of the threads will execute on the driver node. If possible itâ€™s best to use Spark data frames when working with thread pools, because then the operations will be distributed across the worker nodes in the cluster. The MLib version of using thread pools is shown in the example below, which distributes the tasks to worker nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "430841be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[10, 0.8137058470592257], [20, 0.8110705072802111], [50, 0.8333640368138624]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark version\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "# define a function to train a RF model and return metrics \n",
    "def mllib_random_forest(trees, boston_train, boston_test):\n",
    "\n",
    "    # train a random forest regressor with the specified number of trees\n",
    "    rf = RandomForestRegressor(numTrees = trees, labelCol=\"target\")\n",
    "    model = rf.fit(boston_train)\n",
    "\n",
    "    # make predictions\n",
    "    boston_pred = model.transform(boston_test)\n",
    "    r = boston_pred.stat.corr(\"prediction\", \"target\")\n",
    "\n",
    "    # return the number of trees, and the R value \n",
    "    return [trees, r**2]\n",
    "  \n",
    "# run the tasks \n",
    "pool.map(lambda trees: mllib_random_forest(trees, boston_train, boston_test), parameters)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4743d1",
   "metadata": {},
   "source": [
    "#### Pandas UDFs\n",
    "\n",
    "One of the newer features in Spark that enables parallel processing is Pandas UDFs. With this feature, you can partition a Spark data frame into smaller data sets that are distributed and converted to Pandas objects, where your function is applied, and then the results are combined back into one large Spark data frame. Essentially, Pandas UDFs enable data scientists to work with base Python libraries while getting the benefits of parallelization and distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8204b67",
   "metadata": {},
   "source": [
    "If PyArrow has a version older than 1.0.0, install the most recent PyArrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10e97fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-7.0.0-cp38-cp38-macosx_10_13_x86_64.whl (20.2 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20.2 MB 8.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /Users/yuchien/opt/anaconda3/lib/python3.8/site-packages (from pyarrow) (1.20.1)\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-7.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3d86639",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuchien/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/pandas/group_ops.py:81: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(trees=11, r_squared=0.7319703185707604), Row(trees=20, r_squared=0.7647827821222429), Row(trees=50, r_squared=0.7318645764148629)]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# setup the spark data frame as a table\n",
    "boston_sp.createOrReplaceTempView(\"boston\")\n",
    "\n",
    "# add train/test label and expand the data set by 3x (each num trees parameter)\n",
    "full_df = spark.sql(\"\"\"\n",
    "  select *\n",
    "  from (\n",
    "    select *, case when rand() < 0.8 then 1 else 0 end as training \n",
    "    from boston\n",
    "  ) b\n",
    "  cross join (\n",
    "      select 11 as trees union all select 20 as trees union all select 50 as trees)\n",
    "\"\"\")\n",
    "\n",
    "schema = StructType([StructField('trees', LongType(), True),\n",
    "                     StructField('r_squared', DoubleType(), True)])  \n",
    "\n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def train_RF(boston_pd):\n",
    "    trees = boston_pd['trees'].unique()[0]\n",
    "\n",
    "    # get the train and test groups \n",
    "    boston_train = boston_pd[boston_pd['training'] == 1]\n",
    "    boston_test = boston_pd[boston_pd['training'] == 0] \n",
    "        \n",
    "    # create data and label groups \n",
    "    y_train = boston_train['target']\n",
    "    X_train = boston_train.drop(['target'], axis=1)\n",
    "    y_test = boston_test['target']\n",
    "    X_test = boston_test.drop(['target'], axis=1)\n",
    "   \n",
    "    # train a classifier \n",
    "    rf= RFR(n_estimators = trees)\n",
    "    model = rf.fit(X_train, y_train)\n",
    "\n",
    "    # make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    r = pearsonr(y_pred, y_test)\n",
    "    \n",
    "    # return the number of trees, and the R value \n",
    "    return pd.DataFrame({'trees': trees, 'r_squared': (r[0]**2)}, index=[0])\n",
    "  \n",
    "# use the Pandas UDF\n",
    "results = full_df.groupby('trees').apply(train_RF)\n",
    "\n",
    "# print the results \n",
    "print(results.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a05b321",
   "metadata": {},
   "source": [
    "With this approach, the result is similar to the method with thread pools, but the main difference is that the task is distributed across worker nodes rather than performed only on the driver.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718ec5da",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "Thereâ€™s multiple ways of achieving parallelism when using PySpark for data science. Itâ€™s best to use native libraries if possible, but based on your use cases there may not be Spark libraries available. In this situation, itâ€™s possible to use thread pools or Pandas UDFs to parallelize your Python code in a Spark environment. Just be careful about how you parallelize your tasks, and try to also distribute workloads if possible.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
