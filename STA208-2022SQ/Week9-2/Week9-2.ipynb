{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f946ca3a",
   "metadata": {},
   "source": [
    "## Week 9-2: Variational inference\n",
    "\n",
    "_Bo Y.-C. Ning_\n",
    "\n",
    "#### Announcement\n",
    "* Zoom link to final presentations: https://ucdstats.zoom.us/j/95727035665?pwd=N2hOTHlDTzBQbmtRYlBZT3kzWDBxZz09\n",
    "* Password: 208finalÂ \n",
    "\n",
    "#### Last time\n",
    "* Gaussian process\n",
    "\n",
    "#### Today\n",
    "* Variational Inferece\n",
    "\n",
    "#### Reference\n",
    "* Blei, Kucukelbir, and McAuliffe (2017). _Variational Inference: A Review for Statisticians_. JASA: 112:859-877."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c067b961",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "We have learned the Gaussian mixture model with $K$ mixture components. Two methods are introduced so far:\n",
    "\n",
    "- EM algorithm: relatively fast; output point estimators \n",
    "- MCMC algorithm: relatively slow; output draws from posterior distributions\n",
    "\n",
    "The variational inference algorithms try to combine the benifits of both EM and MCMC. It is computationally faster and also provide (approximate) uncertainty quantifications for unknown parameters.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e411cd51",
   "metadata": {},
   "source": [
    "### Variational inference\n",
    "\n",
    "__Definition 1.__ The Kullback-Leibler (KL) divergence between two probability densities $p(x)$ and $q(x)$ is defined as\n",
    "\n",
    "$$\n",
    "\\text{KL}(p(x) || q(x)) = \\int p(x) \\log \\frac{p(x)}{q(x)} dx.\n",
    "$$\n",
    "\n",
    "A few facts:\n",
    "- $\\text{KL}(p(x) || q(x)) \\geq 0$ and it equals to 0 if and only if $p(x) = q(x)$.\n",
    "- $\\text{KL}(p(x) || q(x)) \\neq \\text{KL}(q(x) || p(x))$ (Nonsymmetric)\n",
    "\n",
    "\n",
    "In variational inference (VI), given a family of $\\mathcal{Q}$ of unknown variables $\\Theta = (\\theta_1, \\dots, \\theta_m)$, we find $q^\\star(\\Theta) \\in \\mathcal{Q}$ that \n",
    "\n",
    "$$\n",
    "q^\\star(\\Theta) = \\min_{q(\\Theta) \\in \\mathcal{Q}} \\text{KL}(q(\\Theta) || p(\\Theta|x)),\n",
    "$$\n",
    "\n",
    "where $p(\\Theta|x)$ is the posterior distribution. A typical choice of $q(\\Theta) = \\prod_{i=1}^m q(\\theta_i)$. Then $q(\\Theta)$ is called the mean-field variational posterior, as we will see below. \n",
    "\n",
    "By plugging-in the expression of the Kullback-Leibler divergence in the last display, we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{KL}(q(\\Theta) || p(\\Theta|x)) \n",
    "& = \\mathbb{E}_{x \\sim q} \\log q(\\Theta) - \\mathbb{E}_{x \\sim q} \\log p(\\Theta | x)\\\\\n",
    "& = \\mathbb{E}_{x \\sim q} \\log q(\\Theta) - \\mathbb{E}_{x \\sim q} \\log p(\\Theta, x) + \\color{red}{\\mathbb{E}_{x \\sim q} \\log p(x)}.\n",
    "\\end{align*}\n",
    "\n",
    "The above expression reveals that the KL divergence depends on $\\log p(x)$, which is typically unavailable. One could consider the Gaussian mixture model, $p(x)$ does not have a closed form. \n",
    "\n",
    "Therefore, instead of minimizing the KL divergence, we switch to maximizing \n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{x \\sim q} \\log p(\\Theta, x) - \\mathbb{E}_{x \\sim q} \\log q(\\Theta).\n",
    "$$\n",
    "\n",
    "This is known as the evidence lower bound (ELBO) as it is a lower bound for $p(x)$. To see this, write \n",
    "\n",
    "$$\n",
    "\\text{ELBO}(q) = \\log p(x) - \\text{KL}(q(\\Theta) || p(\\Theta|x)).\n",
    "$$\n",
    "\n",
    "Since KL divergence is nonnegative, we have $\\text{ELBO}(q) \\leq \\log p(x)$. Using One can further write the ELBO as follows:\n",
    "\n",
    "$$\n",
    "\\text{ELBO}(q) = \\mathbb{E}_{x \\sim q} \\log p(x|\\Theta) + \\mathbb{E}_{x \\sim q} \\log \\pi(\\Theta) - \\mathbb{E}_{x \\sim q} \\log q(\\Theta).\n",
    "$$\n",
    "\n",
    "Our goal is to find $q^\\star$ which $q^\\star(\\Theta) = \\arg\\max_{q \\in \\mathcal{Q}} \\text{ELBO}(q)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfb687d",
   "metadata": {},
   "source": [
    "### Mean-field variational inference\n",
    "\n",
    "Of course, one has to specify $\\mathcal{Q}$. One commonly used class is the _mean-field_ variational class defined as \n",
    "\n",
    "$$\n",
    "\\mathcal{Q} = \\Big\\{q: q(\\Theta) = \\prod_{j=1}^m q_j(\\theta_j)\\Big\\}. \n",
    "$$\n",
    "\n",
    "Then for each $\\theta_j$, $\\text{ELBO}(q(\\theta_j)) = \\mathbb{E}_{x \\sim q} \\log p(\\theta_j | \\theta_{-j}, x) - \\mathbb{E}_{x\\sim q}\\log q_j(\\theta_j)$. \n",
    "\n",
    "We estimate $q^\\star(\\Theta)$ by finding $q^\\star_j(\\theta_j)$ for each $j$ using the coordinate ascent variational inference (CAVI) algorithm. \n",
    "The algorithm goes like this:\n",
    "\n",
    "__CAVI method:__\n",
    "\n",
    "1. Input the data $x$ and the model $p(x, \\Theta)$\n",
    "2. Initialize $q_j(\\theta_j)$:\n",
    "3. Repeat:\n",
    "    for $j = 1, \\dots, m$:\n",
    "    \n",
    "    Obtain $q^\\star(\\theta_j) \\propto \\exp \\{\\mathbb{E}(\\log p_j(\\theta_j | \\theta_{-j}, x)\\}$\n",
    "    \n",
    "    Compuate the ELBO: $\\text{ELBO}(q) = \\mathbb{E}\\log p(x,\\Theta) - \\mathbb{E} \\log q(\\Theta)$    \n",
    "4. Stop when the ELBO converges\n",
    "5. Output $q^\\star = \\prod_{j=1}^m q^\\star_j$.\n",
    "\n",
    "\n",
    "Example: Gaussian mixture model (go through [Blei, Kucukelbir, and McAuliffe (2017)](https://arxiv.org/pdf/1601.00670.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab21a81",
   "metadata": {},
   "source": [
    "The ``BayesianGaussianMixture`` uses the CAVI method: see [link](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.BayesianGaussianMixture.html#sklearn.mixture.BayesianGaussianMixture). It also includes the infinite mixture model with the Dirichlet Process!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8b855d",
   "metadata": {},
   "source": [
    "### Concluding remarks:\n",
    "\n",
    "Topics we covered in the class:\n",
    "\n",
    "- Linear methods for regression:\n",
    "\n",
    "      Linear regression, subset selection, ridge regression, lasso, lasso in high-dimension, variations of the lasso method\n",
    "\n",
    "- Classification \n",
    "\n",
    "      Logistic regression, Linear Discriminant Analysis, Quadratic Discriminant Analysis, support vector machine, \n",
    "      K-means, Gaussian mixture models, PCA, factor analysis, PCA in high-dimension\n",
    "        \n",
    "- Nonparametric methods:\n",
    "\n",
    "      Empirical CDF, Histograms, Kernel density estimation, Dirichlet process mixture, B-splines, \n",
    "      Gaussian process\n",
    "        \n",
    "- Algorithms:\n",
    "\n",
    "      Least Angle Regression, Coordinate descent, Gradient descent, Newton's method, EM, MCMC, \n",
    "      variational inference\n",
    "\n",
    "- Theory:\n",
    "\n",
    "      Bias-variance tradeoff, convergence rate for nonparametric methods, random matrix theory, Glivenko-Cantelli, Donsker theorem\n",
    "\n",
    "There are a lot of topics we did not have time to cover. To name a few: Tree-based methods (CART, BART), neural networks, causal inference, methods for handling missing data, reinforcement learning, maniford learning, ... \n",
    "\n",
    "Again, ML is a big field. It is impossible to cover all the topics in an one-quarter class. But I believe the topics you learned from this class will already pave the way for you to understand other (perhaps more complicated) methods/algorithms/theory in the future.\n",
    "\n",
    "__Thanks, and please do not forget to submit your course evaluation before June 2nd!!!__\n",
    "\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
