{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Week 2-2: Ridge regression\n",
    "\n",
    "#### Announcement\n",
    "* HW due this Friday\n",
    "* TA office hour this afternoon 3:00 - 5:00pm\n",
    "* Final report due June 06 @ 11:59pm\n",
    "\n",
    "#### Last time\n",
    "* Subset selection methods\n",
    "\n",
    "#### Today\n",
    "* Ridge regression\n",
    "\n",
    "\n",
    "#### References\n",
    "\n",
    "- Belkin, M., D. Hsu, S. Ma, and S. Mandal (2019). Reconciling modern machine learning practice and the bias-variance trade-off. _Proceedings of the National Academy of Sciences_ 116:15849--15854.\n",
    "- ESL, Chapter 3\n",
    "- Hoerl, A. E. and Kennard, R. W. (1970). Ridge regression: biased estimation for non-orthogonal problems. _Technometrics_, 12:55--67.\n",
    "- Mei, S. and A. Montanari (2020). The generalization error of random features regression: Precise asymptotics and double descent curve. https://arxiv.org/pdf/1908.05355.pdf\n",
    "- Theobald, C. M. (1974). Generalizations of Mean Square Error Applied to Ridge Regression. _JRSSB_. 36:103--106\n",
    "- Wainwright, W. (2019). High-dimensional statistics: A non-asymptotic viewpoint. _Cambridge Series in Statistical and Probabilitistic Mathematics_.\n",
    "- van Wieringen, W. N. (2021). Lecture notes on ridge regression. [https://arxiv.org/pdf/1509.09169.pdf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall subset selection \n",
    "\n",
    "The best subset selection solves\n",
    "\n",
    "$$\\min_{\\beta \\in \\mathbb{R}^p} \\left\\{ \\|Y - X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_0 \\right\\}$$\n",
    "\n",
    "\n",
    "Is $\\|\\beta \\|_0$ a norm? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector norm $\\|\\cdot\\|: \\mathbb{R}^n \\to \\mathbb{R}$, for $a \\in \\mathbb{R}^{n}$: \n",
    "- $\\|a\\| \\geq 0$\n",
    "- $\\|a\\| = 0$ if and only if $a = 0$\n",
    "- homogeneity: $\\|ca\\| = c\\|a\\|$, $c \\geq 0$\n",
    "- triangle inequality: $\\|a + b\\| \\leq \\|a\\| + \\|b\\|$\n",
    "\n",
    "\n",
    "<img src=\"lq-ball.png\" style=\"width: 600px;\"/>\n",
    "\n",
    "Plot of $\\|a\\|_q$-norm: (a) $q=1$, (b) $q=0.75$, (c) $q = 0.5$.\n",
    "\n",
    "(Source: Chapter 7 of High-dimensional statistics by Martin J. Wainwright (2019))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convex relaxation\n",
    "\n",
    "* LASSO (least absolute shrinkage and selection operator) replaces $\\|\\beta\\|_0$ with $\\|\\beta\\|_1$. Solving\n",
    "\n",
    "$$\\min_{\\beta \\in \\mathbb{R}^p} \\left\\{ \\|Y - X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_1 \\right\\}$$\n",
    "\n",
    "* Ridge regression. Solving\n",
    "\n",
    "$$\\min_{\\beta \\in \\mathbb{R}^p} \\left\\{ \\|Y - X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_2^2 \\right\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression \n",
    "\n",
    "<img src=\"ridge-regression.png\" style=\"width: 900px;\"/>\n",
    "\n",
    "Solving $\\min_{\\beta \\in \\mathbb{R}^p} \\left\\{ \\|Y - X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_2^2 \\right\\}$, we obtain \n",
    "$$\n",
    "\\hat \\beta_{\\text{ridge}} = (X'X + \\lambda I_p)^{-1} X'y\n",
    "$$\n",
    "\n",
    "\n",
    "$p < n$\n",
    "\n",
    "- Colinearity\n",
    "\n",
    "- Show $\\hat \\beta_{\\text{ridge}}$ is biased \n",
    "- Derive the variance of $\\hat \\beta_{\\text{ridge}}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"bias.png\" style=\"width: 600px;\"/>\n",
    "\n",
    "Source: [Link](https://www.few.vu.nl/~wvanwie/Courses/HighdimensionalDataAnalysis/WNvanWieringen_HDDA_Lecture234_RidgeRegression_20182019.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"variance.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "<img src=\"contour.png\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"MSE.png\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solving ridge regression using SVD\n",
    "\n",
    "Let $X = UDV'$, the thin SVD for $X$,\n",
    "where $U = (u_1, \\dots, u_p)$, $V = (v_1, \\dots, v_p)'$, and $D = \\text{diag}(d_1, \\dots, d_p)$\n",
    "\n",
    "- $p < n$\n",
    "- $X \\in \\mathbb{R}^{n \\times p}$\n",
    "- $U \\in \\mathbb{R}^{n \\times p}$\n",
    "- $D \\in \\mathbb{R}^{p \\times p}$\n",
    "- $V \\in \\mathbb{R}^{p \\times p}$\n",
    "- $u_k \\in \\mathbb{R}^{n}$\n",
    "- $v_k \\in \\mathbb{R}^{p}$\n",
    "\n",
    "Then the ridge solution is given by\n",
    "$$\n",
    "\\hat \\beta_{\\text{ridge}} = \\sum_{j=1}^p \\frac{d_j v_ju_j'}{d_j^2 + \\lambda} y\n",
    "$$\n",
    "\n",
    "What about when $p > n$? [Bad example](https://anujkatiyal.com/blog/2017/09/30/ml-regression/#.Yk5lc27MIRQ)\n",
    "\n",
    "In python, you can use [this package](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) (solver for nonsparse matrix, use cholesky)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  0.05127781299961498\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 2.33864885e-05, -1.04650148e-04,  3.37335092e-05,  2.15090793e-04,\n",
       "        1.72066669e-04, -1.77999487e-04,  2.26323266e-05,  3.90972968e-05,\n",
       "       -3.21042395e-06, -1.65162587e-04])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "\n",
    "n_samples, n_features = 100, 100000\n",
    "rng = np.random.RandomState(seed=2022)\n",
    "y = rng.randn(n_samples)\n",
    "X = rng.randn(n_samples, n_features)\n",
    "\n",
    "start = timeit.default_timer() # start timer\n",
    "clf = Ridge(alpha=0.2, solver = \"auto\")\n",
    "clf.fit(X, y)\n",
    "stop = timeit.default_timer() # stop timer\n",
    "print('Time: ', stop - start)  \n",
    "clf.coef_[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  1.0505208269969444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 2.33864885e-05, -1.04650148e-04,  3.37335092e-05,  2.15090793e-04,\n",
       "        1.72066669e-04, -1.77999487e-04,  2.26323266e-05,  3.90972968e-05,\n",
       "       -3.21042395e-06, -1.65162587e-04])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = timeit.default_timer() # start timer\n",
    "clf = Ridge(alpha=0.2, solver = \"svd\")\n",
    "clf.fit(X, y)\n",
    "stop = timeit.default_timer() # stop timer\n",
    "print('Time: ', stop - start)  \n",
    "clf.coef_[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  0.06843933499476407\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 2.33864885e-05, -1.04650148e-04,  3.37335092e-05,  2.15090793e-04,\n",
       "        1.72066669e-04, -1.77999487e-04,  2.26323266e-05,  3.90972968e-05,\n",
       "       -3.21042395e-06, -1.65162587e-04])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = timeit.default_timer() # start timer\n",
    "clf = Ridge(alpha=0.2, solver = \"cholesky\")\n",
    "clf.fit(X, y)\n",
    "stop = timeit.default_timer() # stop timer\n",
    "print('Time: ', stop - start)  \n",
    "clf.coef_[0:10]\n",
    "\n",
    "# I found that using the cholesky is the fastest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Degrees of freedom\n",
    "\n",
    "In usual linear regression setting, the degrees of freedom of a model is the number of free parameters $p$. In ridge regression, we define the _effective degress of freedom_ as \n",
    "$$\n",
    "df(\\lambda) = \\text{tr}[X'(X'X + \\lambda I_p)^{-1} X'] = \\sum_{j=1}^p \\frac{d_j^2}{d_j^2 + \\lambda},\n",
    "$$\n",
    "which is a monotone decreasing function of $\\lambda$. \n",
    "\n",
    "- when $\\lambda = 0$, the effective degrees of freedom is $p$;\n",
    "- when $\\lambda \\to \\infty$, the effective degress of freedom $\\to 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connection to Bayesian method (optional, if time permits)\n",
    "\n",
    "In many textbook or lecture notes, they want to connect ridge regression with Bayesian method. Here is why:\n",
    "\n",
    "- Bayes rule: \n",
    "\n",
    "$$\n",
    "\\pi(\\beta | X) = \\frac{p(x|\\beta) \\pi(\\beta)}{\\int p(x|\\beta) \\pi(\\beta) d\\beta}\n",
    "$$\n",
    "\n",
    "- The likelihood function is $p(x | \\beta) = N(y - X\\beta, \\sigma^2I_n)$\n",
    "- Prior for $\\beta$ is $\\pi(\\beta) = N(0, \\tau^2 I_p)$\n",
    "- Then one can show that (let $\\sigma^2 = 1$ and $1/\\tau^2 = \\lambda$\n",
    "$$\n",
    "\\pi(\\beta | X) \\sim N((X'X + \\lambda I_p)^{-1} X'y, (X'X + \\lambda I_p)^{-1})\n",
    "$$\n",
    "- The MAP (maximum a posteriori estimation) is the ridge regression \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression in high-dimensional (double descent phenomenon, active research area)\n",
    "\n",
    "* When $p > n$, interesting thing happens. \n",
    "\n",
    "\n",
    "<img src=\"double-descent.png\" style=\"width: 800px;\"/>\n",
    "\n",
    "\n",
    "Source: [website](https://m-clark.github.io/posts/2021-10-30-double-descent/)\n",
    "\n",
    "\n",
    "* Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal (2019). Reconciling modern machine learning practice and the bias-variance trade-off. _Proceedings of the National Academy of Sciences_ 116:15849--15854.\n",
    "* Song Mei and Andrea Montanari (2020). The generalization error of random features regression: Precise asymptotics and double descent curve. https://arxiv.org/pdf/1908.05355.pdf\n",
    "* Mei's slides [here](https://www.stat.berkeley.edu/~songmei/Teaching/STAT260_Spring2021/Lecture_notes/Lecture_24_DD.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
