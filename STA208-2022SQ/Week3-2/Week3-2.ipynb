{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Week 3-2: Lasso and variants of lasso \n",
    "\n",
    "\n",
    "#### Last time\n",
    "* Lasso\n",
    "* LARs\n",
    "\n",
    "#### Today\n",
    "* Continue with LARS\n",
    "* Coordinate descent\n",
    "* Elastic Net\n",
    "* Adaptive lasso\n",
    "* Group lasso\n",
    "* SCAD, etc...\n",
    "\n",
    "#### References\n",
    "\n",
    "- ESL, Chapter 3\n",
    "- James Sharpnack's lecture notes\n",
    "- Fan, J., and Li, R. (2001), Variable Selection via Nonconcave Penalized Likelihood and Its Oracle Properties. _JASA_ 96: 1348--1360\n",
    "- Friedman, J., T. Hastie, H. Höfling, and R. Tibshirani. (2007) Pathwise Coordinate Optimation. _AOS_ 1:302--332\n",
    "- Yuan, M. and Y. Lin (2006). Model selection and estimation in regression with grouped variables. _JRSSB_ 68:49--67\n",
    "- Ročková, V. and E. I. George. (2018) Spike-and-slab LASSO. _JASA_ 113:431--444\n",
    "- Zou, H. and T. Hastie. (2005), Regularization and variable selection via the elastic net. _JRSSB_ 67:301--320\n",
    "- Zou, H. (2006). Adaptive Lasso and Its Oracle Properties. _JASA_ 101:1418--1429"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erranta\n",
    "\n",
    "1. About one standard error rule\n",
    "\n",
    "Given $$\n",
    "\\hat \\lambda = \\min_{\\lambda \\in \\{\\lambda_1, \\dots, \\lambda_m\\}} \\text{CVErr}(\\hat \\beta_{\\text{lasso}}(\\lambda)) \n",
    "$$\n",
    "Choose $\\hat \\lambda_{\\text{lasso}}$ such that \n",
    "    $$\n",
    "    \\hat \\lambda_{\\text{lasso}} = \\max\\{\\lambda \\in \\{\\lambda_1, \\dots, \\lambda_m\\}, \\quad \n",
    "        \\text{CVErr}(\\hat \\beta_{\\text{lasso}}(\\lambda)) \\leq \n",
    "        \\text{CVErr}(\\hat \\beta_{\\text{lasso}}(\\hat \\lambda)) + \\text{SE}(\\text{CVErr}(\\hat \\beta_{\\text{lasso}}(\\lambda)))\\}\n",
    "    $$\n",
    "    \n",
    "Should be \n",
    "Choose $\\hat \\lambda_{\\text{lasso}}$ such that \n",
    "    $$\n",
    "    \\hat \\lambda_{\\text{lasso}} = \\max\\{\\lambda \\in \\{\\lambda_1, \\dots, \\lambda_m\\}, \\quad \n",
    "        \\text{CVErr}(\\hat \\beta_{\\text{lasso}}(\\lambda)) \\leq \n",
    "        \\text{CVErr}(\\hat \\beta_{\\text{lasso}}(\\hat \\lambda)) + \\text{SE}(\\text{CVErr}(\\hat \\beta_{\\text{lasso}}\\color{red}{(\\hat\\lambda)}))\\}\n",
    "    $$\n",
    "    \n",
    "See Ryan Tibshirani's note: [here](https://www.stat.cmu.edu/~ryantibs/advmethods/notes/highdim.pdf)\n",
    "\n",
    "\n",
    "2. Orthgonal design $X'X = I_p$\n",
    "$$\n",
    "\\hat \\beta_j = \\min_{\\beta_j} \\left\\{\n",
    "\\frac{1}{2} \\left(\\beta_j - \\hat \\beta_j^{OLS}\\right)^2 + \\lambda |\\beta_j|\n",
    "\\right\\}\n",
    "$$\n",
    "\n",
    "- If $\\hat \\beta_j^{OLS} > 0$, then it is reasonable to set $\\beta_j > 0$. Thus taking derivative w.r.t. $\\beta_j$ and then setting it equals to zero, we obtain $\\hat \\beta_j = \\hat \\beta_j^{OLS} - \\lambda$. Thus we get \n",
    "$\\hat \\beta_j = \\max \\left\\{0, \\hat \\beta_j^{OLS} - \\lambda \\right\\}$.\n",
    "- If $\\hat \\beta_j^{OLS} < 0$, then following a similar argument, we get $\\hat \\beta_j = \\min \\left\\{\\hat \\beta_j^{OLS} + \\lambda, 0 \\right\\}$.\n",
    "- Combine the above two result, we get $\\hat \\beta_j = sgn\\left(\\hat \\beta_j^{OLS}\\right)\\left(|\\hat \\beta_j^{OLS}| - \\lambda\\right)_+$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso \n",
    "\n",
    "<img src=\"lasso.png\" style=\"width: 600px;\"/>\n",
    "\n",
    "Lasso, short for 'least absolute selection operator', which shrinks some coefficients and sets others to 0, hence tries to retain the good features of both subset selection and ridge regression.\n",
    "\n",
    "$$\\min_{\\beta \\in \\mathbb{R}^p} \\left\\{ \\|Y - X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_1 \\right\\},$$\n",
    "\n",
    "where $\\|\\beta\\| = \\sum_{j=1}^p |\\beta_j|$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Least Angle Regression (LAR/LARS)\n",
    "\n",
    "1. Standardize predictors and start with residual $r = y - \\bar y$, $\\hat \\beta = 0$\n",
    "2. Find $x_j$ most correlated with $r$\n",
    "3. Move $\\beta_j$ in the direction of $x_j^\\top r$ until the residual is more correlated with another $x_k$\n",
    "4. Move $\\beta_j,\\beta_k$ in the direction of their joint OLS coefficients of $r$ on $(x_j,x_k)$ until some other competitor $x_l$ has as much correlation with the current residual\n",
    "5. Continue until all predictors have been entered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding LAR\n",
    "\n",
    "Recall that we've learned forward stepwise regression. To add one variable at a time. At each step, identify the best variable to include in the model, and then update the least square fit sequentially.\n",
    "\n",
    "LAR uses a similar idead but instead of exploiting the current variable at much as possible, LARS only fits the current variable to a certain level.\n",
    "\n",
    "At first step, similar to the forward stepwise regression, identify the variable most correlated with the response.\n",
    "\n",
    "Then, increase the coefficient for this variable: \n",
    "\n",
    "   - What happens here is that the correlation between this variable and the residual of the fit to decrease (as the coefficient slowly increases)\n",
    "   - stop until another variable has the same absolute correlation (with the residual) as the current one. At this point, both variables are in the active set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each step when a new variable enters the active set, $\\mathcal{S}$, slowly increase the coefficient of the variables in $\\mathcal{S}$ along the direction of the least-square of the current residual on $\\mathcal{S}$, i.e.\n",
    "$$\n",
    "\\beta_\\mathcal{S}(\\alpha) = \\beta_\\mathcal{S}(0) + \\alpha \\delta_{\\mathcal{S}},\n",
    "$$\n",
    "where $\\delta_{\\mathcal{S}} = (X_\\mathcal{S}'X_\\mathcal{S})^{-1} X_\\mathcal{S}'r$, $r = y - X_\\mathcal{S}\\beta_\\mathcal{S}(0)$.\n",
    "$\\beta_\\mathcal{S}(0)$ is the coefficent vector for the variables in $\\mathcal{S}$ at the step. There are $|S| - 1$ zero values.\n",
    "\n",
    "   - Note that the variable which just entered the active set had coefficient 0.\n",
    "   - The correlations between the variables in $\\mathcal{S}$ and the residual $r(\\alpha) = y − X_\\mathcal{S} \\beta_{\\mathcal{S}}(\\alpha)$ decrease at the same time as $\\alpha$ increases\n",
    "    \n",
    "Increase $\\alpha$, until there is another variable which has as much correlation with the residual $r(\\alpha)$ as the ones in the active set $\\mathcal{S}$, at which point, the new variable enters the active set and the iterations restart.    \n",
    "\n",
    "   - At each iteration, there is a value for $\\beta$. This corresponds to a value of $t = \\|\\beta\\|_1$\n",
    "   - At the very beginning, when there was no variable in the active set. One could consider $t = 0$, i.e., $\\|\\beta\\|_1 = 0$ or $\\lambda = \\infty$\n",
    "   - In the end, when all variables are in the active set. This corresponds to $t = \\|\\hat \\beta^{OLS}\\|_1$, i.e., $\\lambda = 0$.\n",
    "   \n",
    "   \n",
    "Further reading: https://b-thi.github.io/pdfs/LARS.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"projection.png\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### LARS with lasso modification\n",
    "\n",
    "4.5 If a non-zero coefficient drops to 0 then remove it from the active set and recompute the restricted OLS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](lars_lasso.png)\n",
    "from ESL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# %load ../standard_import.txt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing, model_selection, linear_model\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Simulate a dataset for lasso\n",
    "np.random.seed(2022)\n",
    "n=100\n",
    "p=1000\n",
    "X = np.random.randn(n,p)\n",
    "X = preprocessing.scale(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-zero's: 17\n"
     ]
    }
   ],
   "source": [
    "## Subselect true active set\n",
    "sprob = 0.02\n",
    "Sbool = np.random.rand(p) < sprob\n",
    "s = np.sum(Sbool)\n",
    "print(\"Number of non-zero's: {}\".format(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Construct beta and y\n",
    "\n",
    "mu = 100.\n",
    "beta = np.zeros(p)\n",
    "beta[Sbool] = mu * np.random.randn(s)\n",
    "\n",
    "eps = np.random.randn(n)\n",
    "y = X.dot(beta) + eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,  -12.10139786,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,  -56.95492021,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "        292.81360414,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,  -22.85577814,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        , -128.33834596,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,   63.00102249,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,  112.78639822,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "        -24.29111753,    6.29991572,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,  -82.95142274,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        , -155.02277835,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        , -136.49505894,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,   96.61915235,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,  176.62608111,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    2.57219574,\n",
       "          0.        ,    0.        ,  116.52757779,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,   93.53046254,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    0.        ,    0.        ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the lasso using `linear_model.lars_path` with the lasso modification (see docstring with ?linear_model.lars_path) \n",
    "\n",
    "Plot the lasso coefficients that are learned as a function of lambda.  You should have a plot with the x-axis being lambda and the y-axis being the coefficient value, with $p=1000$ lines plotted.  Highlight the $s$ coefficients that are truly non-zero by plotting them in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "?linear_model.lars_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Answer to exercise 5.3\n",
    "## Run lars with lasso mod, find active set\n",
    "\n",
    "larper = linear_model.lars_path(X,y,method=\"lasso\")\n",
    "S = set(np.where(Sbool)[0])\n",
    "\n",
    "def plot_it():\n",
    "    for j in S:\n",
    "        _ = plt.plot(larper[0],larper[2][j,:],'r')\n",
    "    for j in set(range(p)) - S:\n",
    "        _ = plt.plot(larper[0],larper[2][j,:],'k',linewidth=.75)\n",
    "    _ = plt.title('Lasso path for simulated data')\n",
    "    _ = plt.xlabel('lambda')\n",
    "    _ = plt.ylabel('Coef')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_it()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving lasso using coordinate descent\n",
    "\n",
    "Another popular method to solve lasso is coordinate descent. See more details [here](https://www.stat.cmu.edu/~ryantibs/convexopt-S15/lectures/22-coord-desc.pdf).\n",
    "\n",
    "#### Coordinate descent (cd)\n",
    "\n",
    "In general, for a function $f(\\beta) = g(\\beta) + \\sum_{j=1}^p h_j(\\beta_j)$ with $g$ is convex and differentiable and each $h_j$ is convex (nonsmooth), the cd method goes like this\n",
    "\n",
    "- __Step 1:__ start with some initial guess $\\beta^{(0)}$ (e.g., OLS estimator or Ridge estimator). \n",
    "- __Step 2:__ Repeat $k = 1, \\dots, K$\n",
    "    - $\\beta_1^{(k)} = arg\\min_{\\beta_1} f(\\beta_1, \\beta_2^{(k-1)}, \\beta_3^{(k-1)}, \\dots, \\beta_p^{(k-1)})$\n",
    "    - $\\beta_2^{(k)} = arg\\min_{\\beta_2} f(\\beta_1^{(k)}, \\beta_2, \\beta_3^{(k-1)}, \\dots, \\beta_p^{(k-1)})$\n",
    "    - $\\beta_3^{(k)} = arg\\min_{\\beta_3} f(\\beta_1^{(k)}, \\beta_2^{(k)}, \\beta_3, \\dots, \\beta_p^{(k-1)})$\n",
    "    - $\\cdots$\n",
    "    - $\\beta_p^{(k)} = arg\\min_{\\beta_p} f(\\beta_1^{(k)}, \\beta_2^{(k)}, \\beta_3^{(k)}, \\dots, \\beta_{p-1}^{(k)}, \\beta_p)$\n",
    "- __STOP:__ if $\\|\\beta^{(k)} - \\beta^{(k-1)}\\|\\leq \\epsilon$ for some small $\\epsilon$.    \n",
    "\n",
    "\n",
    "Notes:\n",
    "\n",
    "1. Order of cycle through coordinates is arbitrary, can use any permutation of $\\{1, \\dots, p\\}$\n",
    "2. Can everywhere replace individual coordinates with blocks of coordinates\n",
    "3. “One-at-a-time” update scheme is critical, and “all-at-once” scheme _does not_ necessarily converge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coordinate descent for lasso\n",
    "\n",
    "Recall lasso \n",
    "$$\n",
    "\\min_{\\beta} \\frac{1}{2}\\|y - X\\beta\\|^2 + \\lambda \\|\\beta\\|_1,\n",
    "$$\n",
    "Q: what is $g(\\beta)$ and what is $h_j(\\beta_j)$ here?\n",
    "\n",
    "Apply CD, minimize $\\beta_j$ one-at-a-time; keep other $\\beta_{-j}$ fixed, where\n",
    "$\\beta_{-j} = (\\beta_1, \\dots, \\beta_{j-1}, \\beta_{j+1}, \\dots, \\beta_p)$\n",
    "\n",
    "$$\n",
    "\\min_{\\beta_j} \\frac{1}{2}\\|y - X_j\\beta_j - X_{-j} \\beta_{-j} \\|^2 + \\lambda (|\\beta_j| + \\|\\beta_{-j}\\|_1),\n",
    "$$\n",
    "Taking the first derivative w.r.t. $\\beta_j$, we get\n",
    "$$\n",
    "X_j'X_j \\beta_j + X_j'(X_{-j} \\beta_{-j} - y) + \\lambda s_j = 0,\n",
    "$$\n",
    "where $s_j = d|\\beta_j| / d \\beta_j$.\n",
    "\n",
    "Denote $z_j = X_j'X_j$ and $\\rho_j = X_j'(X_{-j} \\beta_{-j} - y)$. Solving $\\beta_j$, we get\n",
    "\\begin{align}\n",
    "\\hat\\beta_j = \\left\\{\n",
    "\\begin{array}{cl}\n",
    "\\frac{\\rho_j + \\lambda}{z_j} & \\text{if} \\rho_j < - \\lambda \\\\\n",
    "0 & \\text{if} -\\lambda \\leq \\rho_j \\leq \\lambda \\\\\n",
    "\\frac{\\rho_j - \\lambda}{z_j} & \\text{if} \\rho_j > \\lambda \n",
    "\\end{array}\n",
    "\\right.\n",
    "\\end{align}\n",
    "\n",
    "In short, one can denote \n",
    "$$\n",
    "\\hat \\beta_j = \\text{sgn}(\\rho_j) \\left(\\frac{|\\rho_j| - \\lambda}{z_j}\\right)_+,\n",
    "$$\n",
    "where $(a)_+ = \\max\\{0, a\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coordinate descent in numpy \n",
    "\n",
    "See [website](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hitters dataset\n",
    "\n",
    "df = pd.read_csv('../data/Hitters.csv', index_col=0).dropna()\n",
    "df.index.name = 'Player'\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.Salary\n",
    "\n",
    "# Drop the column with the independent variable (Salary), and columns for which we created dummy variables\n",
    "X_ = df.drop(['Salary', 'League', 'Division', 'NewLeague'], axis=1).astype('float64')\n",
    "# Define the feature set X.\n",
    "X = pd.concat([X_, dummies[['League_N', 'Division_W', 'NewLeague_N']]], axis=1)\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use cross-validate to select the lambda just like any other tuning parameter.  \n",
    "\n",
    "- Sklearn gives you the option of using their fast cross-validation script via `linear_model.LassoCV`, see the documentation.  You can create a leave-one-out cross validator with `model_selection.LeaveOneOut` then pass this to `LassoCV` with the `cv` argument.  Do this, and see what the returned fit and selected lambda are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit the lasso and cross-validate, increased max_iter to achieve convergence\n",
    "loo = model_selection.LeaveOneOut()\n",
    "looiter = loo.split(X)\n",
    "hitlasso = linear_model.LassoCV(cv=looiter,max_iter=2000) \n",
    "hitlasso.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The selected lambda value is {:.2f}\".format(hitlasso.alpha_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitlasso.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compare this to the selected model from forward stagewise regression:\n",
    "\n",
    "```\n",
    "[-0.21830515,  0.38154135,  0.        ,  0.        ,  0.        ,\n",
    "        0.16139123,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "        0.09994524,  0.56696569, -0.16872682,  0.16924078,  0.        ,\n",
    "        0.        ,  0.        , -0.19429699,  0.        ]\n",
    "```\n",
    "\n",
    "This is not exactly the same model with differences in the inclusion or exclusion of AtBat, HmRun, Runs, RBI, Years, CHmRun, Errors, League_N, Division_W, NewLeague_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bforw = [-0.21830515,  0.38154135,  0.        ,  0.        ,  0.        ,\n",
    "        0.16139123,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "        0.09994524,  0.56696569, -0.16872682,  0.16924078,  0.        ,\n",
    "        0.        ,  0.        , -0.19429699,  0.        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\", \".join(X.columns[(hitlasso.coef_ != 0.) != (bforw != 0.)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is the lasso method good?\n",
    "\n",
    "From Ročková and George (2018)\n",
    "\n",
    "_Simulation study:_\n",
    "\n",
    "- $n = 100$, $p = 1000$\n",
    "- Generate $y \\sim N(X\\beta_0, I_n)$\n",
    "    - $X \\sim N(0, \\Sigma)$, $\\Sigma = \\text{bdiag}(\\tilde \\Sigma, \\tilde \\Sigma, \\dots, \\tilde \\Sigma)$; \n",
    "      diagonal of $\\tilde \\Sigma$ is 1 and off-diagonal is $0.9$\n",
    "    - 6 coordinates in $\\beta_0$ are non-zero, the remaining is 0. Nonzero coordinates are at $1, 51, 101, 151, 201, 251$ locations. Values are chosen $\\frac{1}{\\sqrt{3}}\\{−2.5, −2, −1.5, 1.5, 2, 2.5\\}$\n",
    "- Generate 100 independent copies of $y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"comparison.png\" style=\"width: 700px;\"/>\n",
    "\n",
    "- SSL (spike-and-slab lasso), EMVS & Horseshoe are Bayesian methods\n",
    "- MSE: mean square error\n",
    "- FDR: false discovery rate (true value is 0, but get non-zero estimated value)\n",
    "- FNR: false negative rate (true value is non-zero, but get 0)\n",
    "- $\\hat q$: average size of the model\n",
    "- TRUE: number of true model detected\n",
    "- HAM: hamming loss (classification error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Adaptive lasso\n",
    "\n",
    "\n",
    "Why we are not satisfied with lasso?\n",
    "\n",
    "Fan and Li (2001)\n",
    "- Lasso can perform automatic variable selection because the $\\ell_1$-penalty\n",
    "- the lasso shrinkage produces biased estimates for the large coefficients, and thus it could be suboptimal in terms of estimation risk\n",
    "\n",
    "Although lasso selection can be consistent under certain conditions, Zhou (2006) showed that when the condition is violated, lasso variable selection could be inconsistent.\n",
    "\n",
    "A good reference book on this topic is _Martin J. Wainwright. High-dimensional Statistics: A non-asymptotic viewpoint. Cambridge_ (Free download [here](https://www.cambridge.org/core/books/highdimensional-statistics/8A91ECEEC38F46DAB53E9FF8757C7A4E))\n",
    "\n",
    "Zou (2006) proposed the weighted lasso\n",
    "$$\n",
    "\\hat \\beta_{adalasso} = arg\\min_\\beta \\frac{1}{2} \\| y - X\\beta\\|_2^2 + \\lambda \\sum_{j=1}^p \\hat w_j |\\beta_j|,\n",
    "$$\n",
    "where $\\hat w_j = 1/|\\hat \\beta_j|$. $\\hat \\beta_j$ can be 1) OLS estimator; 2) ridge estimator; ...\n",
    "\n",
    "Idea: When $|\\beta_j|$ is large, $\\hat w_j$ is small (penalize less --> less bias); when $|\\beta_j|$ is small, $\\hat w_j$ is large (penalize more).\n",
    "\n",
    "This [website](https://towardsdatascience.com/an-adaptive-lasso-63afca54b80d) provides a tutorial on implementing the adaptive lasso method in python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SCAD (smoothly clipped absolute deviations)\n",
    "\n",
    "_Fan and Li (2001)_\n",
    "\n",
    "A good penalty function should result in an estimator with three properties:\n",
    "1. Unbiasedness: The resulting estimator is nearly unbi- ased when the true unknown parameter is large to avoid unnecessary modeling bias. \n",
    "2. Sparsity: The resulting estimator is a thresholding rule, which automatically sets small estimated coefficients to zero to reduce model complexity. \n",
    "3. Continuity: The resulting estimator is continuous in data to avoid instability in model prediction.\n",
    "\n",
    "\n",
    "<img src=\"scad.png\" style=\"width: 900px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The penalty term for SCAD\n",
    "\n",
    "\\begin{align}\n",
    "h(z| \\lambda, \\gamma) = \\left\\{\n",
    "\\begin{array}{cl}\n",
    "\\lambda |z| & \\text{if} |z| \\leq \\lambda \\\\\n",
    "\\frac{2a \\lambda|z| - z^2 - \\lambda^2}{2(a-1)} & \\text{if} \\lambda < |z| <a\\lambda \\\\\n",
    "\\frac{\\lambda^2 (a+1)}{2} & \\text{if} |z| \\geq a\\lambda\n",
    "\\end{array}\n",
    "\\right.\n",
    "\\end{align}\n",
    "One chooses $a > 2$\n",
    "\n",
    "Comparing to lasso, its penalty term is $\\lambda |z|$. SCAD coincides with the lasso until $|z| = \\lambda$, then\n",
    "smoothly transitions to a quadratic function until $|z| = \\gamma\\lambda$, after which it remains constant for all $|z| > \\gamma\\lambda$\n",
    "\n",
    "\n",
    "__Package__: I did not find a python package for SCAD, but there is a R package [ncvreg](https://cran.r-project.org/web/packages/ncvreg/ncvreg.pdf) for SCAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Elastic Net\n",
    "\n",
    "_Zou and Hastie (2005)_\n",
    "\n",
    "- In the $p > n$ case, the lasso selects at most $n$ variables before it saturates\n",
    "- If there is a group of variables among which the pairwise correlations are very high, then the lasso tends to select only one variable from the group and does not care which one is selected.\n",
    "- The elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together\n",
    "\n",
    "The elastic net solution\n",
    "$$\n",
    "{\\displaystyle {\\hat {\\beta}_{en} } = {\\underset {\\beta }{\\operatorname {argmin} }}(\\|y-X\\beta \\|^{2}+\\lambda _{2}\\|\\beta \\|^{2}+\\lambda _{1}\\|\\beta \\|_{1}).}\n",
    "$$\n",
    "\n",
    "- Can be also solved using coordinate descent\n",
    "\n",
    "__Package__: Elastic net is available in scikit-learn [here](https://scikit-learn.org/stable/modules/linear_model.html#elastic-net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Group lasso\n",
    "\n",
    "_Yuan and Lin (2006)_\n",
    "\n",
    "If we know several variables come in the same (predefined) group. One wants to either force all of them to 0 or non-zero, then one can use the group lasso (say there are $J$ groups)\n",
    "$$\n",
    "\\hat \\beta_{glasso} = arg\\min_\\beta \\left\\{ \\left\\|y - \\sum_{j=1}^J X_j\\beta_j\\right\\|^2 + \\lambda \\sum_{j=1}^J \\|\\beta_j\\| \\right\\},\n",
    "$$\n",
    "when $J = P$, it reduces to lasso. The norm $\\sum_{j=1}^J \\|\\beta_j\\|$ is known as the $\\ell_{2,1}$-norm\n",
    "\n",
    "__Package__: No default package. See implementatio [here](https://github.com/yngvem/group-lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other methods?\n",
    "\n",
    "- MCP (minimax concave penalty): idea is similar to SCAD, but use a different penalty. \n",
    "\n",
    "      Cun-Hui Zhang (2010). Nearly Unbiased Variable Selection Under Minimax Concave Penalty. AOS. 38:894--942\n",
    "\n",
    "- Fused lasso. A generalization of lasso for problems with features that can be ordered in some meaningful way.\n",
    "\n",
    "      Robert Tibshirani et al (2005). Sparsity and smoothness via the fused lasso. JRSSB. 67:91--108\n",
    "\n",
    "- The dantzig selector. Read Ch 3.8.3 of ESL\n",
    "\n",
    "- Spike-and-slab lasso. Derived from the Bayesian perspective. \n",
    "        \n",
    "      Ročková, V. and E. I. George. (2018) Spike-and-slab LASSO. _JASA_ 113:431--444\n",
    "        \n",
    "- Full Bayesian procedure: sparse priors (to be covered later of this course).\n",
    "\n",
    "      Note: Some textbook often says that lasso can be viewed as the Bayes estimate using the double exponential (or Laplace) prior (e.g., on Page 72 of ESL); similar to the ridge regression, using a multivariate normal prior. However, the LASSO (so does ridge) is not fully Bayesian. We will explain later when we discuss \n",
    "      Bayesian methods."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
